{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmL9EiT_RS9k"
   },
   "source": [
    "### Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zo3apvqORS9n",
    "outputId": "5c3bca0d-bddb-4936-c973-bf4fb2e70ce9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataNlp = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', names=['label','body_text'], header=None)\n",
    "dataNlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2Gy-IYGRS9x",
    "outputId": "cfd46026-7938-4ae2-b6eb-4d1e71b002ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4822\n",
       "spam     746\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataNlp['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZUzA6mrRS90"
   },
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q1. Preprocess the data so that stopwords are removed\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def processData(txt):\n",
    "    dataNlp['noPunct'] = txt.apply(lambda x: \"\".join([char.lower() for char in x if char not in string.punctuation]))\n",
    "    #dataNlp['tokenized'] = dataNlp.noPunct.apply(lambda x: re.split('\\W+', x))\n",
    "    dataNlp['tokenized'] = dataNlp.noPunct.apply(lambda x: word_tokenize(x))\n",
    "    dataNlp['noStopwords'] = dataNlp.tokenized.apply(lambda x: [word for word in x if word not in stopword])\n",
    "    dataNlp['lemmatized'] = dataNlp.noStopwords.apply(lambda x: [wnl.lemmatize(word) for word in x])\n",
    "    #return dataNlp.lemmatized\n",
    "    \n",
    "processData(dataNlp.body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>noPunct</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>noStopwords</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>ive been searching for the right words to than...</td>\n",
       "      <td>[ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[ive, searching, right, words, thank, breather...</td>\n",
       "      <td>[ive, searching, right, word, thank, breather,...</td>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>even my brother is not like to speak with me t...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aids...</td>\n",
       "      <td>[even, brother, like, speak, treat, like, aid,...</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>i have a date on sunday with will</td>\n",
       "      <td>[i, have, a, date, on, sunday, with, will]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>[date, sunday]</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          body_text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                             noPunct  \\\n",
       "0  ive been searching for the right words to than...   \n",
       "1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "2  nah i dont think he goes to usf he lives aroun...   \n",
       "3  even my brother is not like to speak with me t...   \n",
       "4                  i have a date on sunday with will   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [ive, been, searching, for, the, right, words,...   \n",
       "1  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "2  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [i, have, a, date, on, sunday, with, will]   \n",
       "\n",
       "                                         noStopwords  \\\n",
       "0  [ive, searching, right, words, thank, breather...   \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "2  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "3  [even, brother, like, speak, treat, like, aids...   \n",
       "4                                     [date, sunday]   \n",
       "\n",
       "                                          lemmatized  body_len  punct%  \n",
       "0  [ive, searching, right, word, thank, breather,...       160     2.5  \n",
       "1  [free, entry, 2, wkly, comp, win, fa, cup, fin...       128     4.7  \n",
       "2  [nah, dont, think, go, usf, life, around, though]        49     4.1  \n",
       "3  [even, brother, like, speak, treat, like, aid,...        62     3.2  \n",
       "4                                     [date, sunday]        28     7.1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Q2. Write down a function that can count percentage of punctuation marks in the text\n",
    "\n",
    "dataNlp['body_len'] = dataNlp['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "def countPunct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")),3)*100\n",
    "\n",
    "dataNlp['punct%'] = dataNlp.body_text.apply(lambda x: countPunct(x))\n",
    "dataNlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW22NaJ3RS93"
   },
   "source": [
    "### Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqdW1B-HRS96",
    "outputId": "f6aae094-4554-4ef5-fce7-16792ddcc5e7"
   },
   "outputs": [],
   "source": [
    "## Q3. For the test and train datasets create a TfIdfVectorizer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    #lem = [wnl.lemmatize(word) for word in tokens if word not in stopword]\n",
    "    lem = [ps.stem(word) for word in tokens if word not in stopword]\n",
    "    return lem\n",
    "\n",
    "tfVect = TfidfVectorizer(analyzer=clean_text)\n",
    "xtfdata = tfVect.fit_transform(dataNlp.body_text)\n",
    "dataVect = pd.DataFrame(xtfdata.toarray(), columns = tfVect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089mi</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ü</th>\n",
       "      <th>üll</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  body_len  punct%         0  008704050406  0089mi  0121  01223585236  \\\n",
       "0   ham       160     2.5  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "1  spam       128     4.7  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "2   ham        49     4.1  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "3   ham        62     3.2  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "4   ham        28     7.1  0.0  0.0           0.0     0.0   0.0          0.0   \n",
       "\n",
       "   01223585334  ...  zindgi  zoe  zogtoriu  zoom  zouk  zyada    é    ü  üll  \\\n",
       "0          0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "1          0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "2          0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "3          0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "4          0.0  ...     0.0  0.0       0.0   0.0   0.0    0.0  0.0  0.0  0.0   \n",
       "\n",
       "   〨ud  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  \n",
       "\n",
       "[5 rows x 8110 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalNlp=pd.concat([dataNlp[['label','body_len','punct%']], dataVect], axis=1)\n",
    "finalNlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqJIRUufRS93"
   },
   "outputs": [],
   "source": [
    "### Q4. Split the whole data set into training and test datasets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(finalNlp.drop(['label'],axis=1), finalNlp.label, test_size=0.2, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.323806</td>\n",
       "      <td>1.023754</td>\n",
       "      <td>0.244392</td>\n",
       "      <td>0.027989</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 100}</td>\n",
       "      <td>0.970852</td>\n",
       "      <td>0.979821</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.975281</td>\n",
       "      <td>0.971910</td>\n",
       "      <td>0.975079</td>\n",
       "      <td>0.003361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.367296</td>\n",
       "      <td>0.346758</td>\n",
       "      <td>0.288475</td>\n",
       "      <td>0.013929</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 200}</td>\n",
       "      <td>0.969731</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.979775</td>\n",
       "      <td>0.973034</td>\n",
       "      <td>0.973034</td>\n",
       "      <td>0.974630</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.620927</td>\n",
       "      <td>0.413381</td>\n",
       "      <td>0.228134</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 100}</td>\n",
       "      <td>0.970852</td>\n",
       "      <td>0.975336</td>\n",
       "      <td>0.978652</td>\n",
       "      <td>0.973034</td>\n",
       "      <td>0.974157</td>\n",
       "      <td>0.974405</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>26.828771</td>\n",
       "      <td>24.545555</td>\n",
       "      <td>0.294763</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 200}</td>\n",
       "      <td>0.968610</td>\n",
       "      <td>0.978700</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.971910</td>\n",
       "      <td>0.975281</td>\n",
       "      <td>0.974405</td>\n",
       "      <td>0.003710</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.460447</td>\n",
       "      <td>0.214714</td>\n",
       "      <td>0.242370</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 100}</td>\n",
       "      <td>0.967489</td>\n",
       "      <td>0.971973</td>\n",
       "      <td>0.976404</td>\n",
       "      <td>0.973034</td>\n",
       "      <td>0.969663</td>\n",
       "      <td>0.971711</td>\n",
       "      <td>0.003029</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "7        8.323806      1.023754         0.244392        0.027989   \n",
       "8       13.367296      0.346758         0.288475        0.013929   \n",
       "10       7.620927      0.413381         0.228134        0.016813   \n",
       "11      26.828771     24.545555         0.294763        0.007340   \n",
       "4        6.460447      0.214714         0.242370        0.014283   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "7               90                100   \n",
       "8               90                200   \n",
       "10            None                100   \n",
       "11            None                200   \n",
       "4               60                100   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "7     {'max_depth': 90, 'n_estimators': 100}           0.970852   \n",
       "8     {'max_depth': 90, 'n_estimators': 200}           0.969731   \n",
       "10  {'max_depth': None, 'n_estimators': 100}           0.970852   \n",
       "11  {'max_depth': None, 'n_estimators': 200}           0.968610   \n",
       "4     {'max_depth': 60, 'n_estimators': 100}           0.967489   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "7            0.979821           0.977528           0.975281   \n",
       "8            0.977578           0.979775           0.973034   \n",
       "10           0.975336           0.978652           0.973034   \n",
       "11           0.978700           0.977528           0.971910   \n",
       "4            0.971973           0.976404           0.973034   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "7            0.971910         0.975079        0.003361                1  \n",
       "8            0.973034         0.974630        0.003586                2  \n",
       "10           0.974157         0.974405        0.002587                3  \n",
       "11           0.975281         0.974405        0.003710                3  \n",
       "4            0.969663         0.971711        0.003029                5  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q5. Create a Ensemble classifier that can predict if the given Text is a Spam or a Ham \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 100, 200], 'max_depth': [30, 60, 90, None]}\n",
    "        \n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1) # n_jobs=-1 for parallelizing search\n",
    "gs_fit = gs.fit(X_train, y_train)\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXXGNXShRS95"
   },
   "source": [
    "\n",
    "### Vectorize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFR4VWHHRS98"
   },
   "source": [
    "### Final evaluation of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0cxztj2RS99"
   },
   "outputs": [],
   "source": [
    "y_pred = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0cxztj2RS99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       955\n",
      "        spam       1.00      0.85      0.92       159\n",
      "\n",
      "    accuracy                           0.98      1114\n",
      "   macro avg       0.99      0.92      0.95      1114\n",
      "weighted avg       0.98      0.98      0.98      1114\n",
      " \n",
      "Accuracy 0.9784560143626571\n"
     ]
    }
   ],
   "source": [
    "## Q6. Evaluate the performance of your model using confusion matrix\n",
    "print(classification_report(y_test,y_pred),\"\\nAccuracy\",accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0cxztj2RS99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(33.0, 0.5, 'True Label')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test,y_pred),annot=True, x)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[955   0]\n",
      " [ 24 135]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix\\n\",confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Random Forest Classifier model gives an accuracy of 0.98 which is really good. High value f1 score is also obtained from the model. Zero ham was predicted as spam and 24 spams were predicted incorrectly as hams. That means only 24 out of 1114 data was wrongly predicted and that too as ham which is justifiable compared to ham predicted as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Spam-Ham Classifier .ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
